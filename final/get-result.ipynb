{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from skimage import io, color\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics import JaccardIndex, Dice\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from folders A and B\n",
    "folder_A = 'test/A'\n",
    "folder_B = 'test/B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU or CPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from directory\n",
    "def load_images_from_folder(folder, is_gray = True):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = io.imread(os.path.join(folder,filename)).astype(np.uint8)\n",
    "        if img is not None:\n",
    "            if is_gray:\n",
    "                images.append(color.rgb2gray(img))\n",
    "            else:    \n",
    "                images.append(img)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_A = load_images_from_folder(folder_A, is_gray=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_B = load_images_from_folder(folder_B, is_gray=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608 608\n"
     ]
    }
   ],
   "source": [
    "print(len(images_A), len(images_B)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5, 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeDetectionDataset(Dataset):\n",
    "    def __init__(self, images_A, images_B):\n",
    "        self.images_A = images_A\n",
    "        self.images_B = images_B\n",
    "        \n",
    "#         self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_A)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_A = torch.tensor(self.images_A[idx], dtype=torch.float32).permute(2, 0, 1) / 255\n",
    "        image_B = torch.tensor(self.images_B[idx], dtype=torch.float32).permute(2, 0, 1) / 255\n",
    "        \n",
    "\n",
    "\n",
    "        return image_A, image_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_Attention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Spatial_Attention, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        q = x.mean(dim=[2, 3], keepdim=True)\n",
    "        k = x\n",
    "        square = (k - q).pow(2)\n",
    "        sigma = square.sum(dim=[2, 3], keepdim=True) / (h * w)\n",
    "        att_score = square / (2 * sigma + np.finfo(np.float32).eps) + 0.5\n",
    "        att_weight = nn.Sigmoid()(att_score)\n",
    "\n",
    "        return x * att_weight\n",
    "\n",
    "class First_DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(First_DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class Conv_With_Attention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv_With_Attention, self).__init__()\n",
    "        out_channels_without_att = out_channels // 2\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels_without_att, kernel_size = 1, stride = 1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels_without_att),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels_without_att, out_channels_without_att, kernel_size = 3, stride = 1, padding = 1, groups= out_channels_without_att // 4, bias=False),\n",
    "            nn.BatchNorm2d(out_channels_without_att),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.attention = Spatial_Attention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.attention(x2)\n",
    "        out = torch.cat([x1, x3], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.Conv = nn.Sequential(\n",
    "            Conv_With_Attention(in_channels, out_channels),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv_With_Attention(out_channels, out_channels),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.Conv(input)\n",
    "\n",
    "\n",
    "class UNet_Siamese(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet_Siamese, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1_1 = First_DoubleConv(in_channels, 32)\n",
    "        self.Conv1_2 = First_DoubleConv(in_channels, 32)\n",
    "        self.Conv2_1 = DoubleConv(32, 64)\n",
    "        self.Conv2_2 = DoubleConv(32, 64)\n",
    "        self.Conv3_1 = DoubleConv(64, 128)\n",
    "        self.Conv3_2 = DoubleConv(64, 128)\n",
    "        self.Conv4_1 = DoubleConv(128, 256)\n",
    "        self.Conv4_2 = DoubleConv(128, 256)\n",
    "        self.Conv5_1 = DoubleConv(256, 512)\n",
    "        self.Conv5_2 = DoubleConv(256, 512)\n",
    "\n",
    "        self.Up5 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.Up_conv5 = DoubleConv(512, 256)\n",
    "\n",
    "        self.Up4 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.Up_conv4 = DoubleConv(256, 128)\n",
    "\n",
    "        self.Up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.Up_conv3 = DoubleConv(128, 64)\n",
    "\n",
    "        self.Up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.Up_conv2 = DoubleConv(64, 32)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(32, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # encoding\n",
    "        c1_1 = self.Conv1_1(x1)\n",
    "        c1_2 = self.Conv1_2(x2)\n",
    "        x1 = torch.abs(torch.sub(c1_1, c1_2))\n",
    "        \n",
    "        c2_1 = self.Maxpool(c1_1)\n",
    "        c2_1 = self.Conv2_1(c2_1)\n",
    "        c2_2 = self.Maxpool(c1_2)\n",
    "        c2_2 = self.Conv2_2(c2_2)\n",
    "        x2 = torch.abs(torch.sub(c2_1, c2_2))\n",
    "        \n",
    "        c3_1 = self.Maxpool(c2_1)\n",
    "        c3_1 = self.Conv3_1(c3_1)\n",
    "        c3_2 = self.Maxpool(c2_2)\n",
    "        c3_2 = self.Conv3_2(c3_2)\n",
    "        x3 = torch.abs(torch.sub(c3_1, c3_2))\n",
    "\n",
    "        c4_1 = self.Maxpool(c3_1)\n",
    "        c4_1 = self.Conv4_1(c4_1)\n",
    "        c4_2 = self.Maxpool(c3_2)\n",
    "        c4_2 = self.Conv4_2(c4_2)\n",
    "        x4 = torch.abs(torch.sub(c4_1, c4_2))\n",
    "\n",
    "        c5_1 = self.Maxpool(c4_1)\n",
    "        c5_1 = self.Conv5_1(c5_1)\n",
    "        c5_2 = self.Maxpool(c4_2)\n",
    "        c5_2 = self.Conv5_2(c5_2)\n",
    "        x5 = torch.abs(torch.sub(c5_1, c5_2))\n",
    "        # x5 = nn.Dropout2d(0.2)(x5)\n",
    "\n",
    "        # decoding\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "        # d5 = nn.Dropout2d(0.2)(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "        # d4 = nn.Dropout2d(0.2)(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "        # d3 = nn.Dropout2d(0.2)(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "        # d2 = nn.Dropout2d(0.2)(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        out = nn.Sigmoid()(d1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet_Siamese(\n",
       "  (Maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv1_1): First_DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv1_2): First_DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv2_1): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv2_2): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv3_1): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv3_2): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv4_1): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv4_2): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv5_1): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv5_2): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up5): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (Up_conv5): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (Up_conv4): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (Up_conv3): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (Up_conv2): DoubleConv(\n",
       "    (Conv): Sequential(\n",
       "      (0): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv_With_Attention(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention): Spatial_Attention()\n",
       "      )\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv_1x1): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet_Siamese(in_channels=3, out_channels=1)\n",
    "# PATH = 'model_2.pth'\n",
    "PATH = 'Haitham.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "images_len = len(images_A)\n",
    "num_of_digits = len(str(images_len))\n",
    "print(num_of_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageName(number):\n",
    "    numberStr = str(number)\n",
    "    while(len(numberStr) < 4):\n",
    "        numberStr = '0'+numberStr\n",
    "    \n",
    "    numberStr = 'final-predicted_255/'+numberStr+'.png'\n",
    "    return numberStr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 37/38 [00:09<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   0   0 ... 255   0   0]\n",
      "  [  0   0   0 ... 255 255   0]\n",
      "  [  0   0   0 ... 255   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ... 255 255   0]\n",
      "  [255 255   0 ... 255 255 255]\n",
      "  [255 255   0 ... 255 255 255]\n",
      "  ...\n",
      "  [  0   0   0 ... 255   0   0]\n",
      "  [  0   0   0 ... 255   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ... 255 255 255]\n",
      "  [  0   0   0 ... 255 255 255]\n",
      "  [  0   0   0 ... 255 255   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ... 255 255 255]\n",
      "  [  0   0   0 ... 255 255   0]\n",
      "  [  0   0   0 ... 255   0   0]]\n",
      "\n",
      " [[  0 255 255 ...   0   0   0]\n",
      "  [255 255 255 ...   0   0   0]\n",
      "  [255 255 255 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:09<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ChangeDetectionDataset(images_A, images_B)\n",
    "data_loader = DataLoader(dataset, batch_size=16)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_total_intersections = 0\n",
    "val_total_unions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    jaccardArr = []\n",
    "    skArr = []\n",
    "    counter = 0\n",
    "    for a, b in tqdm(data_loader):\n",
    "        a, b = a.to(device), b.to(device)\n",
    "        outputs = model(a,b)\n",
    "        outputs = outputs.reshape((-1,256,256))\n",
    "        outputs_binary = (outputs>0.5) * 255\n",
    "           \n",
    "       \n",
    "        for i in range(len(outputs_binary)):\n",
    "           \n",
    "            ###############################################################\n",
    "            \n",
    "            plt.imsave(getImageName(counter), outputs_binary[i].cpu().numpy(), cmap='gray')\n",
    "            counter += 1\n",
    "            \n",
    "            ###############################################################\n",
    "            \n",
    "            # fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            # # Plot the first image\n",
    "            # axes[0].imshow(a[i].cpu().permute(1, 2, 0).numpy())\n",
    "            # axes[0].set_title('Image A')\n",
    "\n",
    "            # # Plot the second image\n",
    "            # axes[1].imshow(b[i].cpu().permute(1, 2, 0).numpy())\n",
    "            # axes[1].set_title('Image B')\n",
    "\n",
    "            # axes[2].imshow(outputs_binary[i].cpu().reshape(256,256), cmap='gray')\n",
    "            # axes[2].set_title('Predicted')\n",
    "\n",
    "            # # Remove the axis labels\n",
    "            # for ax in axes:\n",
    "            #     ax.axis('off')\n",
    "\n",
    "            # # Adjust the spacing between subplots\n",
    "            # plt.tight_layout()\n",
    "\n",
    "            # # Show the plot\n",
    "            # plt.show()\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
